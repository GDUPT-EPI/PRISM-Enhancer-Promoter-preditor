目标：在无标签下游环境中，采用纯负优化（只施加否定性校准），不重新训练主干，不使用0/1标签。基于已训练旁路于有限细胞系学到的特性/共性分布，直接对 EP 互作输出的分布进行“拒绝式迁移（Rejection-based Transfer）”，仅根据“不像已知环境”的证据减少互作概率。

---

### 一、 EP 互作的概率分解模型 (The Probability Decomposition)

我们将 $P(y=1 | x_E, x_P, c)$ 分解为两个相互独立的因子：

$$ P(y=1) \approx P(\text{Compatible} | x_E, x_P) \times P(\text{Active} | x_E, x_P, M_{cell}) $$

1.  **$P(\text{Compatible})$ [由 Backbone 负责]**：
    *   这是序列层面的硬约束（Hard Constraint）。
    *   例如：Enhancer 和 Promoter 是否拥有匹配的转录因子结合位点？DNA 环化（Looping）的结构基础是否存在？
    *   **数学性质**：这是 **跨细胞系不变 (Invariant)** 的。你的 Backbone 已经在预训练中学会了这一点，这是它能达到 60% 的基础。

2.  **$P(\text{Active})$ [由 Bypass + FiLM 负责]**：
    *   这是环境层面的软约束（Soft Constraint）。
    *   例如：在这个细胞系中，这些位点是否处于开放染色质区域（Open Chromatin）？相关的 TF 是否表达？
    *   **数学性质**：这是 **环境敏感 (Context-Sensitive)** 的。这正是 60% -> 75% 的增量来源。

结论：下游适应必须冻结 Backbone，仅在隐变量层面进行分布校准；采用“纯负优化”，只对“不匹配环境”的样本施加拒绝惩罚，不做正向强化。

---

### 二、 为什么直接使用 0/1 更新不可行？ (The Failure of Binary Supervision)

假设下游任务是一个全新的细胞系 $C_{new}$。
如果是一个正样本 $(x_E, x_P, y=1)$：
*   模型需要 $P(\text{Compatible}) \uparrow$ 且 $P(\text{Active}) \uparrow$。
*   这没问题，梯度会强化两者。

**但如果是一个负样本 $(x_E, x_P, y=0)$：**
*   这才是问题的关键。负样本有两种情况：
    *   **Case A (结构不匹配)**：序列根本不搭。此时 $P(\text{Compatible}) \approx 0$。
    *   **Case B (环境抑制)**：序列完美匹配，但染色质关闭。此时 $P(\text{Compatible}) \approx 1$，但 $P(\text{Active}) \approx 0$。
*   在 Hard Negative Mining 中，**Case B 占绝大多数**。
*   如果你直接用 $y=0$ 回传梯度更新 Backbone，你会强行压低 $P(\text{Compatible})$。
*   **数学后果**：模型学会了“在这个细胞系里，这个 Motif 组合是错的”。但这违反了生物化学原理（Motif 结合是物理属性）。当你换一个细胞系（该区域开放）时，模型就会误判。

**结论**：在下游适应时，**必须锁定 $P(\text{Compatible})$（即冻结 Backbone）**，仅允许梯度去寻找使得 $P(\text{Active}) \approx 0$ 的环境参数 $M_{cell}$。

---

### 三、 适应性训练的数学目标：能量最小化 (Energy Minimization)

我们不优化参数，而是根据训练期的分布直接计算拒绝惩罚，对输出分布做后验校准。

定义系统的 **自由能 (Free Energy)** $F(x, M)$。
对于下游的一批无标签数据 $X_{batch}$，我们假设它们服从某种潜在的生物学规律。我们寻找最优的 $M^*$ 来解释这些观测数据。

#### 1. 互作一致性势能 (Interaction Consistency Potential)
EP 互作不是随机的，它具有稀疏性和聚集性。
Backbone 输出的特征向量 $h_E, h_P$ 包含序列信息。
$$ \mathcal{L}_{align} = - \sum_{i} \text{Sim}(h_{E_i}, h_{P_i}) \cdot \mathcal{G}(M) $$
这不够。我们需要利用你 `decouple.py` 中的 $I$ (Interaction) 分量。
在 Bypass 中，我们有一个 $z_I$。
对于同一个 batch 的数据，如果 Backbone 认为它们“很像互作”（$P(\text{Compatible})$ 高），而当前的 $M$ 导致 $P(\text{Active})$ 很低，这就产生了 **认知失调 (Cognitive Dissonance)** 或 **高能量状态**。

纯负校准目标：对每个样本 $x$，构造拒绝项 $R(x)$，使得最终输出满足
$$ \hat{y}_{\text{final}}(x) = \text{clamp}\big( \hat{y}_{\text{base}}(x) - \lambda_{\text{neg}} \cdot R(x),\ 0,\ 1 \big) $$
其中 $\hat{y}_{\text{base}}$ 为 Backbone+FiLM 的原始输出概率（或其对数几率的平滑映射），$\lambda_{\text{neg}}>0$ 为负惩罚强度。

*   **$\mathcal{H}(\hat{y})$ (熵最小化)**：
    *   如果 $M$ 是错误的（例如把 K562 当成了 GM12878），FiLM 会进行错误的调制，导致 Backbone 的特征变得混乱，最终预测值 $\hat{y}$ 会徘徊在 0.5 左右（不确定）。
    *   如果 $M$ 是正确的，它会精准地通过 FiLM 抑制那些“本细胞系不开放”的噪音，放大真正的信号，使得 $\hat{y}$ 趋向 0 或 1。
    *   **这就是 60% -> 75% 的数学原理：通过正确的 $M$ 过滤掉 Case B 类的假阳性，大幅提升 Precision，从而提升 AUC/AUPR。**

拒绝项来源于已学到的分布流形，不引入新的优化变量。

---

### 四、 针对 EP 互作的鲁棒性最大化设计

纯负拒绝校准（不做正向强化）：

1) 特性分布的拒绝度（基于多原型的最近邻密度）：
GraphContext 持有每个已知细胞系的特性原型集合 $\{p_{c,k}\}$。对样本 $x$ 的特性向量 $z_F(x)$ 定义各原型的相似度
$$ w_{c,k}(x) = \exp\Big(-\frac{\|z_F(x) - p_{c,k}\|^2}{\tau_F}\Big) $$
令全域最大相似度 $\rho_F(x) = \max_{c,k} w_{c,k}(x)$。拒绝度定义为
$$ R_F(x) = 1 - \rho_F(x) $$
其意义是：如果 $z_F$ 不像任何已知细胞系的特性原型，则该样本在当前环境下更可能被“否决”。

2) 共性分布的拒绝度（可选）：
在训练期可估计共性子空间的包络，例如以训练集的 $z_G$ 近似均值与协方差构造马氏距离，记拒绝度 $R_G(x) \in [0,1]$。若无已存统计，可略去该项以保持简洁。

3) 纯负融合：
记 Backbone+FiLM 的原始输出为 $\hat{y}_{\text{base}}(x)$，最终输出为
$$ \hat{y}_{\text{final}}(x) = \text{clamp}\big( \hat{y}_{\text{base}}(x) - \lambda_{\text{neg}} \cdot (\alpha_F R_F(x) + \alpha_G R_G(x)),\ 0,\ 1 \big) $$
在“纯负优化”设定中，$\alpha_G$ 可取 0，仅使用 $R_F$ 进行校准。

此法无需任何新训练或标签，仅依赖训练期学到的原型分布；它是严格的“拒绝式迁移”：只在负证据出现时收缩 EP 概率分布，不对正例做额外拉升，避免分布偏移导致的假阳性膨胀（如阈值过低时 Recall=1.0 而 Precision 很低的情况）。

### 五、 总结

落实要点：

1. 冻结 Backbone；在评估环节读取旁路的 $z_F$ 与 GraphContext 的原型集合，直接计算 $R_F$。
2. 输出校准仅做减法：$\hat{y}_{\text{final}} = \hat{y}_{\text{base}} - \lambda_{\text{neg}} R_F$；不做任何加法或积分增强，保证“纯负优化”。
3. 阈值选择可继续用宏观最优阈值，但校准后概率分布的尾部将被压缩，预计 Precision 与 AUPR 将提升，Recall 下降到合理水平，从而改善 F1 与 AUC。
