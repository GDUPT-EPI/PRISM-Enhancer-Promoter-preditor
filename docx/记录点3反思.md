# CBAT 修复与反思记录

## 1. 分析修复记录点3：修改前(记录2) vs 修改后(记录3)

### 修改前（记录点2：一维金字塔 + 软稀疏）
- **核心机制**：
  - 使用多尺度一维卷积（Pyramid Conv）提取局部特征。
  - 使用带温度系数的软稀疏注意力（Soft Sparse Attention）替代全连接，保留 Top-k 的梯度但抑制噪声。
  - 引入窗口掩码（Window Mask）限制局部交互。
- **表现**：
  - Precision 显著提升（减少了假阳性）。
  - Recall 回落正常水平。
  - 在未见细胞系（NHEK）上有轻微提升，但仍受限于局部窗口假设，长程交互能力受损。

### 修改后（记录点3：局部上下文编码 + 原型路由注意力）
- **核心机制**：
  - **LCE (Local Context Encoder)**：使用 Depthwise Conv 提取 Motif 级特征，替代了金字塔卷积。
  - **PRA (Prototype Routing Attention)**：彻底抛弃了直接的 $E-P$ 交互。
    - **路由机制**：$E$ 写入全局原型（Prototypes），$P$ 从原型中读取；反之亦然。
    - **信息瓶颈**：所有交互必须通过 $K=64$ 个原型向量中转。
- **表现**：
  - **AUPR/AUC 下滑**：相比记录2和Baseline，指标明显下降（如 GM12878 AUPR 0.66 -> 0.65）。
  - **置信度分化**：模型更倾向于给出高置信度（0或1），但判别力（Ranking）变差。
  - **泛化未达预期**：虽然设计初衷是解耦，但在未见细胞系上因信息丢失导致效果不佳。

---

## 2. 分析与标准 Cross Attn 的区别

| 特性 | 标准 Cross Attn | CBAT (记录点3: Prototype Routing) | 区别影响 |
| :--- | :--- | :--- | :--- |
| **交互路径** | $E \leftrightarrow P$ 直接点积 | $E \to \text{Proto} \to P$ | CBAT 引入了强烈的**信息瓶颈**。 |
| **信息容量** | $O(L^2)$ 交互对 | $O(L \times K)$ (K=原型数) | CBAT 极度压缩了信息，丢失了序列特异性的细微互作信号。 |
| **归纳偏置** | 弱（全连接） | 强（聚类/离散化） | CBAT 假设所有互作都可以归类为 K 种模式，这在生物学上过于简化。 |
| **抗噪性** | 差（容易过拟合噪声） | 强（过滤掉非原型信号） | CBAT 确实过滤了噪声，但也过滤了有效信号。 |

---

## 3. 分析为什么提升不明显（反思）

1.  **过度压缩导致“语义坍缩”**：
    -   Enhancer 和 Promoter 的互作往往依赖于特定的序列模体组合（Motif Combinations）。将长序列（~400bp）压缩为对 64 个原型的投票，丢失了**位置信息**和**组合细节**。
    -   E和P“未曾谋面”，仅通过“中间人”（原型）传话。如果原型词表不够丰富或训练不充分，就像两个人用只有64个单词的语言交流，无法表达复杂含义。

2.  **原型的“平均化”效应**：
    -   原型倾向于学习数据集中的“平均模式”（Common Patterns）。对于决定 E-P 互作的关键**特异性突变**或**稀有模体**，原型可能无法覆盖，导致模型对关键差异不敏感。这就是为什么 AUC（排序能力）下降的原因。

3.  **缺乏直接的校验机制**：
    -   在标准 Attention 中，Query 和 Key 的点积直接校验了匹配度。在路由机制中，Query 匹配的是 Prototype，Key 也是匹配 Prototype。如果两者都匹配同一个“宽泛”的原型（例如都富含 GC），模型就判定互作，但实际上可能序列细节并不匹配。这导致了**高置信度的错误判决**。

4.  **LCE 的局限性**：
    -   虽然 LCE 提取了局部特征，但缺乏像 Transformer 那样的全局整合能力。在进入路由层之前，序列特征可能还没有足够的高层语义。

**总结**：记录点3 矫枉过正。为了追求鲁棒性（通过原型去噪），牺牲了太多的表达能力（Expression Power）。**真正的鲁棒性不应来自“看不见细节”，而应来自“理解了共性结构但保留了细节验证”。**

---

## 4. 下一步方向（痛定思痛）

我们需要一个**“既有全局视野（Baseline优势），又有结构化引导（记录3初衷）”**的方案。

-   **回归直接交互**：必须保留 $E-P$ 的直接 Attention 通路，保证细节的校验。
-   **原型作为“锚点”而非“瓶颈”**：
    -   不要强迫信息流经原型。
    -   而是用原型来**引导**注意力：只有当 $E$ 和 $P$ 在语义上（相对于原型）也匹配时，才允许建立强连接。
    -   即：**Attention = Raw Interaction + Semantic Consistency**。
-   **双流混合机制**：
    -   **Detail Stream**: 处理序列细节。
    -   **Concept Stream**: 处理细胞系共性/原型。
