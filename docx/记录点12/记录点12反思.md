记录点12反思：从 Meta‑Batch 坍缩到 CellBatch 稳定的真正含义

一、现象复盘：为什么记录点12“看起来好了一点”

1. 训练日志对比
- 记录点11（Meta‑Batch + CED‑Net + 蒸馏 + 能量耗散）：
  - 训练 AUPR 快速飙升到 0.90+，AUC 约 0.87 左右（log/记录点11训练日志.log:26-41）。
  - domain‑kl/test：历史结果中 AUPR≈0.51–0.53，AUC≈0.51–0.53，F1 极低（docx/历史结果.log:113-121）。
  - Push、Pull 等竞争项在训练后期几乎恒为 0，Student 与 Opportunist 之间不再有有效博弈。
- 记录点12（CellBatch + 同构批采样）：
  - 训练 AUPR 从 0.56 逐步上升到约 0.76（log/记录点12训练日志.log:26-41），上升速度明显更“正常”，不再出现记录点11那种前几轮就暴涨到 0.90+ 的异常。
  - domain‑kl/test：AUPR 能上到约 0.75 后停滞，但无法继续突破，且 Pull 长期接近 0，Student 竞争几乎不起作用（由实验现象与历史结果综合判断）。

2. 从曲线形态看出的差异
- 记录点11的训练 AUPR 是典型的“虚高”：训练集曲线快速贴近 1，测试集却接近随机。
- 记录点12在训练集上的 AUPR 增长更缓，说明梯度不再由某些极端捷径主导；但测试集 AUPR 在 ≈0.75 的平台上停住，说明模型学到了一种“温和但不深刻”的规律：比完全坍缩好，但仍然没有突破第二基线（记录点4）的天花板。

3. Pull 恒为 0 反映的真实结构问题
- Pull 的理论意义：当 Student 在某些样本上明显比 Opportunist 差时，通过 Pull 项鼓励 Student 向 Opportunist 学习那些“有用但不必然错误”的局部模式。
- 记录点12 中 Pull 长期接近 0，意味着：
  - 要么 Opportunist 的损失整体并不比 Student 更小，二者在大多数样本上持平或优势极弱；
  - 要么由于 APL 门限或实现细节，绝大多数样本不满足“Student 明显落后”的触发条件。
- 结合训练日志中 Main/Domain Loss 的变化，可以判断：当前的 CED‑Net 实际上已经退化为“几乎只有 Student + 对抗 + 稀疏正则”的单流网络，Opportunist 的存在感极低，自然也无法推动真正的“竞争式修正”。

4. AUPR 停在 ≈0.75 的含义
- 记录点4（第二基线）的 AUPR 在 0.71–0.72 区间（docx/历史结果.log:43-51）。
- 记录点12 虽然比记录点8/9/10/11 的完全坍缩状态明显要好，但依然没有形成真正意义上的“鲁棒怪物”：
  - 内势 U_I 的排序能力被多轮蒸馏与对抗反复扰动，已经不再是纯粹的“序列物理引擎”。
  - 环境阻抗 R_E 虽然通过 CellBatch 的同构采样获得了更清晰的统计特征，但 Teacher‑Student‑Prototype 这条支路在训练目标上依然只是“附加约束”，而不是推理时真正主导校准的核心。

二、回到方法论：记录点11/12 真正的问题在哪里

1. 能量耗散系统的数学形式是对的，实现却错位了
- 理论形式：P(y=1)=σ((U_I−R_E)/T)，其中
  - U_I：仅由 EP 序列决定的内生物理势能，是跨细胞系不变的排序基准（docx/记录点4方案.md:24-37）。
  - R_E：仅由环境/细胞系特征决定的非负阻抗，负责对 U_I 进行方向性的抑制（docx/记录点4方案.md:53-77）。
- 记录点11/12 的实现：
  - U_I 由 FourierEnergyKAN(z_I) 生成，但 z_I 本身已经深度卷入 CED‑Net 的 Push‑away 与蒸馏梯度，失去了“绝对不变量”的角色（models/PRISMModel.py:454-486, 771-776）。
  - 当 DISTILL_ENABLED=True 时，R_E 不再由单一的 resistance_generator(z_F) 产生，而是通过 Teacher + Student1 + Prototype 混合生成 R_S，并直接进入 U_I−R_E 的主路（models/PRISMModel.py:505-533）。
- 结果：原本应该“纯净”的 U_I 和“外在”的 R_E 同时被卷入复杂博弈，最终能量 E_total=U_I−R_E/T 的数值分布在训练后期被多重损失共同压扁，出现历史结果中 AUPR≈0.51、AUC≈0.51 的平凡解（docx/历史索引.md:93-99）。

2. CED‑Net 思想本身没错，但被绑死在主能量路径上
- CED‑Net 的目标（学生竞争机制.md）：
  - Teacher 在原型流形上提供环境先验；
  - Student1 在任务损失 + 蒸馏 + Pull/Push 下学会鲁棒环境编码；
  - Student2 作为“机会主义者”，逼迫 Backbone 抛弃捷径特征。
- 合理的用法：
  - CED‑Net 主要作用于 R_E（或更一般的环境编码 M），通过蒸馏与竞争把环境特征空间“打磨干净”；
  - U_I 的通路应尽量不被 CED‑Net 梯度直接扰动，只接受任务损失和必要的轻量正交约束。
- 记录点11/12 的实际做法：
  - z_I 作为所有下游操作（U_I、BypassDecoupler、Student1/2）的共同输入，等价于把 U_I 也拉入了 CED‑Net 游戏；
  - 竞争损失与 KL 蒸馏通过 ced_net_losses 和 compute_competition_loss 回流，实际上对 z_I、z_F、z_G 都产生了混合梯度（models/layers/distill.py:217-252, 255-276；models/PRISMModel.py:771-803）。
- 这直接违背了“U_I 是物理不变量”的初衷，也是记录点11 在测试集上完全坍缩的根本原因：当过多复杂目标叠加在同一潜在空间上时，模型最安全的选择就是输出接近常数的能量，让所有约束同时“满意但无用”。

3. Meta‑Batch vs CellBatch：采样策略修正了输入，但没有修正目标
- 记录点11 使用 Meta‑Batch（log/记录点11训练日志.log:10），每个物理 Batch 由多个细胞系 Block 混合，这会让 Student 看到的 Context 在统计意义上接近杂合噪声，已在记录点8、9反思中详细分析（docx/记录点8反思.md:12-21；docx/记录点9反思.md:18-24）。
- 记录点12 改为 CellBatch（log/记录点12训练日志.log:10-18），即每个 Batch 只包含单一细胞系：
  - Teacher/Student 可以在更干净的上下文上对齐；
  - R_E 的分布不再一刀切地收缩为全局常数。
- 但训练目标仍然是“在同一条 z_I 数据流上同时最小化任务损失 + 蒸馏 + 竞争 + 对抗 + 稀疏正则 + 正交约束”（models/PRISMModel.py:711-812），推理时虽然在 predict.py 中尝试利用 Student1 推断 R_S 并做“方向性校准”（predict.py:375-401），但主干前向已经在训练阶段被迫适应“平均安全”的策略，难以在 OOD 细胞系上展现真正的纠偏能力。
- 换句话说，CellBatch 只是让模型“不那么混乱”，但没有触及 U_I 与 R_E 角色错位的核心矛盾，所以 AUPR 只能从完全坍缩恢复到“中等偏平庸”的水平，而无法超越记录点4。

三、训练流 vs 推理流：记录点12 仍然存在的断裂

1. 训练阶段：所有东西都卷在同一条通路
- 在 PRISMBackbone.forward 中：
  - z_I 从 Anchor 池化得到后，立即进入 U_I 生成、GRL 域对抗、BypassDecoupler、Teacher/Student 等一系列操作（models/PRISMModel.py:454-505）。
  - 若 DISTILL_ENABLED：
    - Teacher 给出 target_dist、R_T、R_protos、M_T（models/layers/distill.py:159-183）。
    - Student1/2 在 (z_I_detach, z_F) 上输出 logits，生成 R_S、R_O，并通过 prob_s/prob_o 构造 Pull/Push 损失（models/PRISMModel.py:523-555, 771-803）。
  - 最终 E_total=U_I−R_E，prob=σ(E_total/T)，主损失直接用这条概率和标签计算（models/PRISMModel.py:547-555, 711-717）。
- 也就是说：训练时的“主预测路径”已经在内部使用了蒸馏后得到的 R_E，且 E_total 的统计分布被各种损失共同约束到了一个狭窄区间。

2. 推理阶段：试图在已被“削平”的 U_I 上做二次纠偏
- predict.evaluate 中：
  - 先调用 build_support_set，对每个细胞系通过整集样本估计一个平均 R_cell（基于 Student1 或静态 R_E）（predict.py:275-315）。
  - 评估时再次前向，取出 U_I，然后：
    - 基线分支：logits=(U_I−R_cell)/T，作为“保守路径”（predict.py:360-371）。
    - 学生分支：重新调用 student1 与 teacher.get_all_prototypes_and_resistance，估计 R_S，并用 (U_I−R_S)/T 生成 student_probs（predict.py:375-401）。
  - 最终按细胞系选择 AUPR 较高的一支用于全局评估（predict.py:420-462）。
- 但由于：
  - U_I 已在训练阶段被迫适应某种“平均稳定解”，排序结构已经被 CED‑Net 损失严重侵蚀；
  - R_S 在训练期间主要参与的是蒸馏与竞争，而不是“显式用来校准 U_I 以提升 OOD AUPR/AUC”的目标；
  - predict.py 的分支选择只是事后的经验选择器，无法弥补训练阶段目标错位带来的根本损伤。
- 因此，从“是否按照方案去准确对环境阻抗纠偏”的角度看：
  - 训练阶段并没有严格落实“U_I 只学共性、R_E 只学特性”的正交解耦；
  - 推理阶段虽然在形式上做了环境纠偏，但建立在一个已经被博弈削平的 U_I 之上，无法发挥应有的效果。

四、模式坍缩的根本原因：不是蒸馏本身，而是“多目标共用一条通路”

1. 记录点8–12 的共同模式
- 每次失败后，我们都在增加新的结构或损失：
  - 元学习上下文、Graph Teacher、CED‑Net、正交约束、Push/Pull、Manifold Align、CellBatch……
  - 但这些新增目标大多被放在同一条潜变量 z_I 或同一能量公式 E_total 上，导致系统整体趋向“最安全的平凡解”：输出近似常数，使所有约束的梯度同时变小。
- 这正是历史索引中对记录点11的总结（docx/历史索引.md:93-99），记录点12 只是通过采样策略将“完全坍缩”缓解为“半坍缩”，本质问题仍未解决。

2. 记录点12 的“相对好转”只是症状缓解
- CellBatch 的引入让 Student/Teacher 看到的上下文变得清晰，阻止了一部分极端模式坍缩：
  - 训练 AUPR 不再虚高；
  - 测试 AUPR 回到了 0.75 左右。
- 但由于核心设计依旧是“U_I 与 R_E 在同一条特征流上共享几乎所有训练信号”，即便采样再合理，模型也只能学到一个“温和折中解”，而不是“U_I 强、R_E 可调”的正交解耦结构。

五、针对记录点12的结论：保留什么，推翻什么

1. 值得保留的部分
- CellBatch 同构采样：
  - 这是让 Student 从 EP 集分布中推断环境的重要前提，应在后续方案中保留并进一步系统化。
- PRISMBackbone 中基于 Anchor 的 z_I 提取与 FourierEnergyKAN：
  - 这部分在记录点4已证明其在 AUPR/AUC 上的稳定性，应继续作为“序列物理引擎”的核心。
- CED‑Net 中 Teacher + Prototype + R_protos 的结构：
  - 有潜力作为环境阻抗 R_E 的“先验图谱”，但必须从主能量通路剥离出去，变为相对独立的环境模块。

2. 必须推翻或彻底重构的部分
- 将 Student1/2 的蒸馏与竞争损失直接叠加到同一条 z_I→E_total 路径上的做法：
  - 需要把 U_I 与 R_E 的训练目标显式分离，避免再让 Push/Pull、KL 等复杂项直接打在 U_I 上。
- 在训练前向中依赖 Student1 实时生成 R_E 并直接参与主预测的模式：
  - Student1 更适合作为“环境表示学习器”，而不是在训练过程中就被绑死为当前 R_E 的唯一来源。
- 用单一温度 T 统一刻画所有细胞系的能量尺度，却忽略不同环境阻抗分布可能需要不同校准的问题：
  - 至少在设计上要明确：T 是 U_I 侧的全局标尺，R_E 只负责非负偏移，不能再通过 T 与 R_E 共同吸收所有不一致。

六、为记录点13铺路：思想没错，工程落地要大改

1. 蒸馏与学生竞争不是根本问题
- 从 学生竞争机制.md 与 解耦对抗算子.md 的数学设计看：
  - 将 Teacher 的全局原型图与 Student 的局部分布推断对齐，本身是合理的；
  - 用 Push‑away 主动拆解 Opportunist 的捷径，有助于提升 U_I 与 R_E 的物理纯度。
- 真正的问题在于实现：
  - 我们把这些高阶机制全部堆在同一条前向流上，让 U_I 在训练中承担了过多不相干的任务。

2. 记录点13 的明确方向
- 后续必须机械而严格地执行以下原则：
  - U_I 路径尽量简化：只负责学习 EP 序列的内在排序，不参与环境博弈；
  - R_E 路径专门负责环境阻抗：Teacher/Student/Prototype 的所有复杂蒸馏、Push/Pull 只作用在 R_E 或其中间表示 M 上；
  - 训练与推理的能量公式必须完全一致：训练时就按照推理时的 P(y=1|U_I,R_E)=σ((U_I−R_E)/T) 来优化，不能再出现“训练一个能量，推理另一个能量”的错位。
- 只有在这样的重构下，CellBatch、CED‑Net 和解耦对抗算子才有机会真正发挥作用，让模型在未见细胞系上做到“靠序列学共性、靠环境编码做纠偏”，而不是再次滑向模式坍缩。

