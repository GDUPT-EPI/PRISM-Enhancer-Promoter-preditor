# 记录点4反思：指标提升与过度置信的矛盾

## 指标现象
- AUPR/AUC较记录点2/3全面抬升（如 Colon AUPR=0.7225, AUC=0.7378），说明排序总体变好，但伴随明显的高阈值与极端置信。
- 阈值显著升高（多数≈0.9），且在 NHEK 出现 Recall≈1.0、Precision≈0.50 的失衡（Threshold=0.5318），典型的“置信上移、召回虚高、精度受损”。

## 与记录点2/3的结构差异
- 引入双向跨序分支与锚点：`E←P` 与 `P←E` 并行，锚点参与序列融合（models/PRISMModel.py:382-447）。
- 使用两级 CBAT 交互：多尺度卷积 + 局部块注意力 + 软稀疏注意力 + 双门控（models/layers/attn.py:600-888）。
- 输出头改为 `U_I` 能量生成器（FourierEnergyKAN）并加入域对抗清洗（GRL+判别器）（models/PRISMModel.py:259-278, 456-475）。

## 与标准 Cross Attention 的关键区别
- 标准 Cross Attn：直接 `Q·K^T` + Softmax + `V`，无结构偏置（models/layers/attn.py:11-77）。
- 记录点4 CBAT：
  - 在注意力 logits 叠加“列偏置”`log_w`（来自稀疏评分），形成对所有 Query 的 Key 先验（models/layers/attn.py:367-370）。
  - 加入多尺度卷积与窗口/块掩码、双门控融合，路径更深更非线性（models/layers/attn.py:651-674, 681-688, 707-714, 716-723）。
  - 池化温度与方向门控使输出分布进一步调制（models/PRISMModel.py:438-446）。

## 问题成因分析
1. 列偏置过强导致“先验压倒匹配”：`attn_logits += log_w`为列全局偏置，稀疏权重尖锐时会跨 Query 强推若干 Key，破坏 `Q·K` 的细节校验与行内竞争（models/layers/attn.py:367-370）。
2. 输出能量无标度约束：`U_I_generator`直接产生未校准的 logits，缺少温度/向量标度，易导致概率整体上移（models/PRISMModel.py:465-471）。
3. 对抗清洗叠加门控放大：GRL促使 `z_I`去域信息，但与门控凸融合叠加时可能把“确定性信号”推得更极端，配合池化 Softmax易放大峰值（models/PRISMModel.py:438-446, 456-466）。
4. 稀疏温度与熵约束不足：`sparsemax`虽保留连续性，但缺少强熵/退火约束，训练后期权重趋于单点化，造成 Recall虚高、Precision下降（models/layers/attn.py:356-365, 392-401）。

## 结论与修复思路
- 去偏注意力：将列偏置从“加到logits”改为“作用在Value或作为小幅加性正则”，并引入小系数 `α_bias≤0.2` 与剪裁，保留 `Q·K`主导地位。
- 输出校准：在 `U_I` 前增加可学习温度/向量标度（Vector/Temperature scaling），并以校准损失（Brier/Entropy penalty）约束过度置信。
- 稀疏退火与熵正则：对 `sparse_temp`做退火策略，引入显式熵下界或 `entmax(α≈1.3-1.5)`，避免单峰化。
- 训练期校验：增加基于验证集的阈值与校准监控，确保 Recall/Precision平衡，不让阈值随训练漂移到极端。

> 本反思的核心是：让“细节匹配(Q·K)”重新成为主判据，稀疏与能量仅作为温和的引导与刻画，而非主导，使排序提升与概率校准并存。
