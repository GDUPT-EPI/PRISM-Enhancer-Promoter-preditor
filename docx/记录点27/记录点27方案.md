# 记录点27 方案：几何硬约束特征调制网络 (Geometrically Hard-constrained Feature Modulation Network, GH-FMN)

## 核心洞察（一句话）

**细胞系环境对EP互作的调节本质是稀疏特征选择，但必须通过双重几何约束（交换对齐+对比不变性）、自适应稀疏度与连续性保障来实现架构级的硬约束，避免方案与实现的断层，从而获得可解释、可外推的细胞系不变性。**

## 问题的数学形式化

### 输入与输出空间
- 输入序列对：$x = (E, P) \in \mathcal{X} \subset \mathbb{V}^{L_E} \times \mathbb{V}^{L_P}$，DNA序列（词汇表 $\mathbb{V}$）。
- 细胞系标签：$c \in \mathcal{C}$，但假设存在（或可学习）连续表观特征 $v_c \in \mathbb{R}^m$。
- 标签：$y \in \{0,1\}$ 表示EP互作。
- 观测分布：$(x,c,y) \sim P_{\text{data}}$，训练细胞系 $\mathcal{C}_{\text{train}}$，测试细胞系 $\mathcal{C}_{\text{test}} \cap \mathcal{C}_{\text{train}} = \emptyset$。

### 当前瓶颈的数学描述
记录点26失败暴露了三个数学缺陷：

1. **实现断层缺陷**：方案空间 $\mathcal{S} = \{f_{\theta}(x,c) = \sigma(\langle w, \phi(x) \odot g_{\psi}(c) \rangle)\}$ 与实现空间 $\mathcal{I} = \{f'_{\theta}(x,c) = \sigma((U_I(\phi(x)) - R_E(c))/T)\}$ 不匹配，导致 $\dim(\mathcal{I}) \gg \dim(\mathcal{S})$，容量失控。

2. **约束软弱缺陷**：对齐损失 $\mathcal{L}_{\text{align}}$ 作为软约束，权重 $\alpha=0.5$ 不足以抗衡主损失 $\mathcal{L}_{\text{task}}$（权重1.0），梯度竞争失败。

3. **外推断裂缺陷**：能量分解使用离散ID嵌入 $R_E(c) = h(\text{Embedding}(c))$，破坏细胞系流形连续性，无法外推。

### 生物学机制假设（修正版）
1. **序列特征基序库**：DNA序列蕴含潜在转录因子结合基序，编码为特征向量 $z = \phi(x) \in \mathbb{R}^d$。
2. **细胞系特异性特征可及性**：细胞系 $c$ 的染色质状态决定基序可及性，对应稀疏门控向量 $g_c \in \Delta^{d-1}$（概率单纯形，$\sum_i g_{c,i} = 1$）。
3. **调制机制**：实际起作用的特征为 $z \odot g_c$（逐元素乘法），不可及基序 ($g_{c,i} \approx 0$) 被掩蔽。
4. **细胞系连续性**：表观特征 $v_c$ 连续，映射 $v_c \to g_c$ 连续（Lipschitz连续）。
5. **自适应稀疏性**：不同细胞系可能调节不同数量的基序，稀疏度 $s_c = \|g_c\|_0$ 应可自适应学习。
6. **几何不变性**：给定互作状态 $y$，调制特征 $z \odot g_c$ 的分布应与 $c$ 无关：$P(z \odot g_c \mid y, c) = P(z \odot g_c \mid y)$。

## 解决方案

### 数学描述

#### 1. 几何硬约束特征调制架构

**序列编码器**（降维瓶颈）：
\[
\phi: \mathcal{X} \rightarrow \mathbb{R}^d, \quad z = \phi(x), \quad d = 256 \ (\text{从768降低})
\]
使用现有CBAT架构但最终投影到 $d$ 维，创造信息瓶颈。

**细胞系连续编码器**：
- 若有显式表观特征矩阵 $V \in \mathbb{R}^{|\mathcal{C}| \times m}$：$\psi(c) = \text{MLP}(v_c) \in \mathbb{R}^{h}$
- 若无：$\psi(c) = \text{AttnPool}(\{\phi(x_i) \mid x_i \in S_c\})$，$S_c$ 为 $c$ 的支持集（$k=32$个随机样本）

**自适应稀疏门控生成器**：
\[
f: \mathbb{R}^h \rightarrow \Delta^{d-1}, \quad g_c = \text{Sparsemax}\left(\frac{\text{MLP}(\psi(c))}{\tau_c}\right)
\]
其中 $\tau_c = \sigma(\text{MLP}_{\tau}(\psi(c))) \in [0.1, 10.0]$ 为可学习温度，控制稀疏度。低温→更稀疏。

**特征调制与预测**：
\[
z_{\text{mod}} = z \odot g_c \quad \text{（逐元素乘法）}
\]
\[
P_\theta(y=1|x,c) = \sigma\left(\langle w, z_{\text{mod}} \rangle + b\right)
\]

#### 2. 双重几何约束损失

**交换对齐损失（特征级硬约束）**：
对于批次内所有同标签样本对 $(i,j)$，$y_i = y_j$：
\[
\mathcal{L}_{\text{align}} = \frac{1}{|\mathcal{P}|} \sum_{(i,j) \in \mathcal{P}} \| z_i \odot g_{c_i} - z_j \odot g_{c_j} \|^2
\]
其中 $\mathcal{P} = \{(i,j): y_i = y_j, c_i \neq c_j\}$ 仅跨细胞系对齐，强制 $I(z \odot g_c; c \mid y) \to 0$。

**对比不变性损失（表示级硬约束）**：
对于每个anchor样本 $i$，构建：
- 正样本：同标签、不同细胞系的 $j$（$y_j = y_i, c_j \neq c_i$）
- 负样本：不同标签的 $k$（$y_k \neq y_i$）

\[
\mathcal{L}_{\text{contrast}} = -\frac{1}{B} \sum_{i=1}^B \log \frac{\exp(\text{sim}(z_i \odot g_{c_i}, z_j \odot g_{c_j})/\tau_{\text{cl}})}{\sum_{k \in \mathcal{N}_i} \exp(\text{sim}(z_i \odot g_{c_i}, z_k \odot g_{c_k})/\tau_{\text{cl}})}
\]
其中 $\text{sim}(u,v) = \frac{u^\top v}{\|u\|\|v\|}$，$\tau_{\text{cl}}=0.1$，$\mathcal{N}_i$ 包含批次内所有负样本。

**双重约束的几何解释**：
- $\mathcal{L}_{\text{align}}$：强制同类样本的调制特征**重合**（减少方差）
- $\mathcal{L}_{\text{contrast}}$：强制异类样本的调制特征**分离**（增加间距）
- 共同作用：在调制特征空间形成紧致的类条件分布，与 $c$ 无关。

#### 3. 自适应稀疏度与连续性保障

**稀疏性正则**：
\[
\mathcal{L}_{\text{sparse}} = \frac{1}{|\mathcal{C}_{\text{batch}}|} \sum_{c \in \mathcal{C}_{\text{batch}}} \|g_c\|_1
\]
但 $\|g_c\|_1 = 1$（Sparsemax性质），改用**熵正则**控制稀疏度：
\[
\mathcal{L}_{\text{entropy}} = -\frac{1}{|\mathcal{C}_{\text{batch}}|} \sum_{c \in \mathcal{C}_{\text{batch}}} \sum_{i=1}^d g_{c,i} \log(g_{c,i} + \epsilon)
\]
最小化熵→更稀疏（one-hot），最大化熵→更均匀。我们目标中等稀疏，故用**熵目标正则**：
\[
\mathcal{L}_{\text{ent-target}} = \left( H(g_c) - H_{\text{target}} \right)^2, \quad H_{\text{target}} = \log(\sqrt{d})
\]
对应约 $\sqrt{d} = 16$ 个活跃维度（$d=256$时）。

**平滑性约束**：
对于批次内细胞系对 $(c,c')$：
\[
\mathcal{L}_{\text{smooth}} = \frac{1}{|\mathcal{P}_c|} \sum_{(c,c') \in \mathcal{P}_c} \exp\left(-\frac{\|v_c - v_{c'}\|^2}{\sigma^2}\right) \cdot \|g_c - g_{c'}\|^2
\]
其中 $\sigma^2 = \text{median}\{\|v_c - v_{c'}\|^2\}$，$\mathcal{P}_c$ 为细胞系对。

#### 4. 主任务损失
沿用 AdaptiveIMMAXLoss（继承项目）：
\[
\mathcal{L}_{\text{task}} = \mathcal{L}_{\text{IMMAX}}
\]

#### 5. 总损失函数
\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \alpha \mathcal{L}_{\text{align}} + \beta \mathcal{L}_{\text{contrast}} + \gamma \mathcal{L}_{\text{ent-target}} + \delta \mathcal{L}_{\text{smooth}}
\]
初始权重：$\alpha=1.0$（对齐与主任务同等重要），$\beta=0.5$，$\gamma=0.2$，$\delta=0.1$。

### 核心机制

#### 1. 为什么双重几何约束有效？
历史方案失败因为不变性约束是"软"的（损失函数），容易被主任务损失淹没。GH-FMN通过：

- **架构硬约束**：调制 $z \odot g_c$ 是前向传播的必经之路
- **损失硬约束**：$\mathcal{L}_{\text{align}}$ 直接最小化跨细胞系特征距离
- **对比硬约束**：$\mathcal{L}_{\text{contrast}}$ 在表示空间强制细胞系无关的类分离

三重硬约束确保不变性不被绕过。

#### 2. 自适应稀疏度的信息论解释
设 $g_c$ 为基序可及性的概率向量，其熵 $H(g_c)$ 衡量不确定性。目标熵 $H_{\text{target}} = \log(\sqrt{d})$ 对应：
- 足够稀疏以提供特征选择（$\sqrt{d} \ll d$）
- 足够灵活以适应不同细胞系需求
- 信息量 $I(g_c; c) = H(g_c) - H(g_c|c)$ 最大化，使 $g_c$ 携带细胞系信息，但 $z \odot g_c$ 与 $c$ 条件独立于 $y$。

#### 3. 连续性保障的外推理论
对于未知细胞系 $c'$：
- 若有 $v_{c'}$：$g_{c'} = f(\psi(v_{c'}))$ 直接计算
- 若无：$v_{c'} = \text{AttnPool}(\{\phi(x_i): x_i \in S_{c'}\})$，$S_{c'}$ 为测试样本

由于 $\psi$ 和 $f$ 连续，相似细胞系产生相似 $g_c$，支持平滑外推。数学上，这实现了细胞系流形 $\mathcal{M}_c$ 到门控流形 $\mathcal{M}_g$ 的连续嵌入。

#### 4. 降维瓶颈的泛化理论
将 $z$ 维度从768降至256：
- 降低VC维：$d_{\text{VC}} \propto d$
- 降低Rademacher复杂度：$\hat{\mathfrak{R}}_n(\mathcal{F}) \propto \sqrt{d}$
- 提高泛化边界紧致性

但通过门控 $g_c$，有效维度进一步降至 $\approx \sqrt{d}=16$，双重降低复杂度。

### 物理/生物对应

#### 1. 特征调制 → 染色质可及性的物理实现
$z_i$：特定TF结合基序的亲和力（由序列决定）。
$g_{c,i}$：该基序在细胞系 $c$ 染色质中的**可及性概率**，由核小体占据、组蛋白修饰等决定。
$z_i \cdot g_{c,i}$：**有效结合强度**，物理上正比于TF占据概率。

#### 2. 双重几何约束 → 进化保守的调控逻辑
- **交换对齐**：尽管染色质状态不同，活跃的增强子-启动子对应暴露相似基序组合。
- **对比不变性**：互作与非互作对的基序模式应可区分，且区分规则跨细胞系保守。

#### 3. 自适应稀疏度 → 细胞类型特异的调控复杂度
不同细胞类型可能调节不同数量的通路：
- 终末分化细胞：少数核心通路，高稀疏度
- 多能干细胞：多通路保持可塑性，较低稀疏度
自适应 $\tau_c$ 允许此变异。

#### 4. 连续外推 → 细胞分化轨迹
$v_c$ 编码细胞在分化轨迹上的位置，$g_c$ 沿轨迹平滑变化，对应分化中染色质可及性的连续重编程。

## 实施路线图

### 第一阶段：架构实现（确保无断层）

#### 1. 降维序列编码器
修改 `PRISMBackbone` 最终层：
```python
self.feature_proj = nn.Linear(OUT_CHANNELS, 256)  # 768 -> 256
self.feature_norm = nn.LayerNorm(256)
```
$z = \text{LayerNorm}(\text{Linear}_{768\to256}(z_{\text{orig}}))$

#### 2. 细胞系连续编码器
```python
class CellLineEncoder(nn.Module):
    def __init__(self, feature_dim=0, hidden_dim=128, output_dim=256):
        # feature_dim: 表观特征维度，0表示使用支持集
        self.use_feature = feature_dim > 0
        if self.use_feature:
            self.feature_proj = nn.Sequential(
                nn.Linear(feature_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, output_dim)
            )
        else:
            self.support_encoder = nn.MultiheadAttention(output_dim, num_heads=4, batch_first=True)
            self.support_norm = nn.LayerNorm(output_dim)

    def forward(self, cell_ids, cell_features=None, support_samples=None):
        # cell_features: [B, m] 或 None
        # support_samples: [B, k, 256] 或 None
        if self.use_feature and cell_features is not None:
            v = self.feature_proj(cell_features)  # [B, output_dim]
        else:
            # support_samples: [B, k, 256]
            attn_out, _ = self.support_encoder(
                support_samples.mean(dim=1, keepdim=True),  # query: 均值 [B, 1, 256]
                support_samples,  # key
                support_samples   # value
            )
            v = self.support_norm(attn_out.squeeze(1))  # [B, 256]
        return v
```

#### 3. 自适应稀疏门控生成器
```python
class AdaptiveSparseGating(nn.Module):
    def __init__(self, input_dim=256, feature_dim=256):
        super().__init__()
        self.gating_mlp = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim)  # 输出d维logits
        )
        self.temp_mlp = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()  # 输出[0,1]
        )
        # 温度映射: [0,1] -> [0.1, 10.0]
        self.temp_scale = 9.9
        self.temp_shift = 0.1

    def forward(self, v_c):
        logits = self.gating_mlp(v_c)  # [B, d]
        temp = self.temp_mlp(v_c) * self.temp_scale + self.temp_shift  # [B, 1]
        g = sparsemax(logits / temp)  # [B, d]
        return g, temp.squeeze()
```

#### 4. 调制分类头
```python
class ModulationClassifier(nn.Module):
    def __init__(self, feature_dim=256):
        super().__init__()
        self.classifier = nn.Linear(feature_dim, 1)

    def forward(self, z, g_c):
        z_mod = z * g_c  # [B, d]
        logits = self.classifier(z_mod)  # [B, 1]
        return torch.sigmoid(logits.squeeze())
```

### 第二阶段：损失模块实现

#### 1. 交换对齐损失
```python
class SwapAlignmentLoss(nn.Module):
    def forward(self, z_mod, labels, cell_ids):
        # z_mod: [B, d], labels: [B], cell_ids: [B]
        loss = 0.0
        B = z_mod.size(0)
        for i in range(B):
            for j in range(i+1, B):
                if labels[i] == labels[j] and cell_ids[i] != cell_ids[j]:
                    loss += F.mse_loss(z_mod[i], z_mod[j])
        return loss / max(1, B*(B-1)/2)
```

#### 2. 对比不变性损失
```python
class ContrastiveInvarianceLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature

    def forward(self, z_mod, labels, cell_ids):
        # z_mod: [B, d], 已归一化
        z_norm = F.normalize(z_mod, p=2, dim=1)
        sim_matrix = torch.mm(z_norm, z_norm.t())  # [B, B]

        loss = 0.0
        B = z_mod.size(0)
        for i in range(B):
            pos_mask = (labels == labels[i]) & (cell_ids != cell_ids[i])
            neg_mask = (labels != labels[i])

            if pos_mask.any() and neg_mask.any():
                pos_sim = sim_matrix[i, pos_mask]
                neg_sim = sim_matrix[i, neg_mask]

                numerator = torch.exp(pos_sim / self.temperature).sum()
                denominator = torch.exp(neg_sim / self.temperature).sum() + numerator
                loss += -torch.log(numerator / denominator)

        return loss / max(1, B)
```

### 第三阶段：训练策略

#### 1. 课程学习三阶段
- **Phase 1 (1 epoch)**：冻结门控，$g_c = \mathbf{1}/d$（均匀），训练序列编码器和分类器。
- **Phase 2 (2-3 epochs)**：解冻门控，逐步增加 $\alpha: 0.1 \to 1.0$，$\beta: 0.1 \to 0.5$。
- **Phase 3 (剩余)**：全量训练，每epoch评估验证集，早停patience=3。

#### 2. 采样策略
- 使用 `CellBatchSampler` 保持批次内细胞系纯度（同细胞系样本连续）。
- 每细胞系缓存 $k=32$ 个样本特征用于支持集编码（若无表观特征）。

#### 3. 监控指标
- **核心**：训练/验证 AUPR、AUC
- **门控质量**：稀疏度 $\|g_c\|_0/d$、熵 $H(g_c)$、温度 $\tau_c$
- **对齐效果**：$\mathcal{L}_{\text{align}}$ 值、类内跨细胞系特征距离
- **外推平滑**：相似细胞系门控余弦相似度

### 第四阶段：推理适配

#### 1. 未知细胞系处理
- 若有表观特征：直接计算 $g_{c'}$
- 若无：从测试细胞系随机采样 $k$ 个样本，提取特征，计算支持集编码 $v_{c'}$，生成 $g_{c'}$

#### 2. 阈值校准
在验证集上优化阈值，最大化F1。

#### 3. 门控可视化
输出热图：细胞系 × 基序维度，验证稀疏模式和连续性。

### 第五阶段：失败应对预案

#### 1. 门控坍缩
若 $g_c$ 退化为全0或全1：增加熵目标权重 $\gamma$，检查温度学习。

#### 2. 对齐失败
若 $\mathcal{L}_{\text{align}}$ 不降：增加批次多样性（确保每批有多个细胞系），增大 $\alpha$。

#### 3. 过拟合持续
- 进一步降低 $d$（256→128）
- 增加Dropout率
- 提前早停

#### 4. 外推失败
检查 $v_c$ 连续性：若无表观特征，增加支持集大小 $k$；增加平滑性权重 $\delta$。

## 预期行为与理论保障

### 成功标志
1. **泛化差距缩小**：训练 AUPR ≈ 0.85-0.90，验证 AUPR ≥ 0.75，差距 ≤ 0.15。
2. **门控质量合理**：稀疏度 10-30%（25-75活跃维度），熵 $H(g_c) \approx \log(16) \approx 2.77$。
3. **对齐有效**：$\mathcal{L}_{\text{align}}$ 稳定下降，跨细胞系类内距离 < 类间距离。
4. **外推平滑**：相似细胞系门控余弦相似度 > 0.8。

### 理论优势
1. **架构-设计对齐**：消除历史断层，方案与实现一致。
2. **几何硬约束**：双重约束确保不变性不被绕过。
3. **自适应复杂度**：降维+稀疏门控双重控制容量。
4. **连续性保障**：支持集编码实现未知细胞系外推。

### 生物学可解释性
1. $z_i$：基序 $i$ 的序列亲和力。
2. $g_{c,i}$：细胞系 $c$ 中基序 $i$ 的可及性概率。
3. $z_i \cdot g_{c,i}$：有效结合强度。
4. 可视化：细胞系 × 基序热图揭示细胞类型特异调控网络。

---

**记录点27的本质创新在于：从历史失败中提取"断层"教训，构建架构与设计严格对齐的特征调制网络；通过双重几何约束、自适应稀疏度、连续性保障三重机制，实现数学优雅性与工程严谨性的统一；为突破AUPR 0.75瓶颈提供兼具理论保证与实施可行性的路径。**