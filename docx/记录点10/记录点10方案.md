# CBAT 记录点10修复方案：回归 CED-Net 与解耦对抗算子

## 1. 核心纠偏：回归设计原点

**严正声明**：废弃之前偏离方向的“流形对齐”方案，严格回归 `学生竞争机制.md` 和 `解耦对抗算子.md` 确立的 **Co-Evolutionary Dual-Stream Network (CED-Net)** 架构。

**当前病灶诊断**：
模式坍缩 (Recall=1.0, Precision=0.5) 的本质不是“流形没对齐”，而是 **S2 (机会主义者) 过于强大且不受控，导致 S1 (集大成者) 放弃了抵抗，Teacher (稳压器) 失效**。系统陷入了一个低效的纳什均衡点：所有模型都预测为正类。

## 2. 修复架构：CED-Net 完整版

### 2.1 角色重申与职责强化
- **Teacher (T)**：
  - **输入**：大 Batch (N=128)，包含多个细胞系。
  - **职责**：利用大样本统计特性，生成稳定的全局 Footprint $F_T$。
  - **更新**：EMA from S1 (不参与 BP)。
- **Student 1 (S1, The Master)**：
  - **输入**：小 Batch (N=64)。
  - **职责**：在预测准确率上压制 S2，同时逼近 $F_T$。在 S1 优于 S2 时，**主动拆解** S2 依赖的特征。
  - **核心能力**：通过 IMMAX Loss 学习鲁棒特征。
- **Student 2 (S2, The Opportunist)**：
  - **输入**：同 S1。
  - **职责**：**贪婪拟合**当前 Batch 的局部模式。它是 S1 的磨刀石。
  - **状态**：目前 S2 过于“聪明”，利用了某些捷径（如全选策略）获得了较低的 Loss，误导了 S1。

### 2.2 关键机制修复 (The Missing Pieces)

#### A. 恢复 Push-away Loss (主动拆解过拟合)
这是解决模式坍缩的关键武器。当 S1 判断正确而 S2 错误（或 S1 更置信）时，我们要**最大化 S2 的 Loss**（仅通过共享 Backbone 反传）。
$$
\mathcal{L}_{push} = -\frac{1}{|\mathcal{I}_{better}|} \sum_{i \in \mathcal{I}_{better}} \text{IMMAX}_{elem}(\hat{y}_i^{(S2)}, y_i)
$$
- **作用**：迫使 Backbone 放弃那些 S2 依赖的、但被 S1 证明是不可靠的特征（即“捷径”）。

#### B. 激活 APL (Adaptive Pacing Lock)
防止 S2 跑得太快或 S1 躺平。
- **计算性能比** $\rho = \text{EMA}(\mathcal{L}_{S1}) / \text{EMA}(\mathcal{L}_{S2})$。
- **动态调节**：
  - 若 $\rho \ll 1$ (S2 遥遥领先)：**抑制 S2 更新** (scale gradients by 0.1)，给 S1 喘息机会。
  - 若 $\rho > 1$ (S1 领先)：**加大 S2 攻击力度**，迫使 S1 进化。

#### C. 正交解耦算子 (Orthogonal Decoupling)
严格实现 `解耦对抗算子.md` 中的数学约束，防止特征纠缠。
$$
\mathcal{L}_{orth} = \| Z_G^T Z_F \|_F^2 + \| Z_G^T Z_I \|_F^2 + \| Z_F^T Z_I \|_F^2
$$
- **$Z_G$ (Global)**: 跨细胞系共性。
- **$Z_F$ (Specific)**: 细胞系特异性（由旁路网络生成）。
- **$Z_I$ (Interaction)**: 序列固有互作势能。
**预测公式**：$P(y=1) = \text{FiLM}(Z_I, Z_F)$，即环境 $Z_F$ 调制 固有势能 $Z_I$。

## 3. 实施步骤

### 步骤 1: 重构 `PRISMModel.py` (CED-Net 结构)
- 移除之前的临时代码。
- 明确定义 `Teacher`, `Student1`, `Student2` 子模块。
- 确保三者共享 `PRISMBackbone` (Encoder)，但拥有独立的 `Head`。
- 实现 **旁路解耦模块 (Bypass Decoupler)**，生成 $Z_G, Z_F, Z_I$。

### 步骤 2: 实现 `IMMAXLoss` 与 `CEDLoss`
- 在 `models/losses.py` 中完整实现 `学生竞争机制.md` 中的损失函数：
  - `Task Loss` (S1, S2)
  - `Align Loss` (S1-T)
  - `Pull-in Loss` (S1 < S2)
  - `Push-away Loss` (S1 > S2) **(Crucial!)**
  - `Orthogonal Loss`

### 3.3 训练循环适配 (Training Loop)
- 在 `PRISM.py` 中集成 APL 逻辑。
- 计算 $\rho$，动态调整 `optimizer` 的 param groups 学习率或梯度缩放。

## 4. 预期修正结果
- **打破全选僵局**：Push-away Loss 会惩罚 Backbone 提取那些导致 S2 盲目自信的特征（如只看 GC 含量而不看序列模体）。
- **恢复 Precision**：随着 S2 的捷径被拆解，S1 必须寻找更本质的特征（$Z_I$ 与 $Z_F$ 的正确交互），从而提升 Precision。
- **解耦验证**：$Z_F$ 应能准确分类细胞系，而 $Z_G$ 无法分类。

## 5. Todo List 更新
1.  **[High]** 重写 `PRISMModel.py`，严格对应 CED-Net 三角色与旁路解耦结构。
2.  **[High]** 实现 `CEDLoss` (含 Push-away, Pull-in, Align) 和 `IMMAXLoss`。
3.  **[Medium]** 在 `PRISM.py` 中实现 APL (自适应步调锁)。
4.  **[High]** 检查并确保数据加载器输出正确的 Context 信息供旁路使用。
