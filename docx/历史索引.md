**历史经验记录（上次修复与反思）**
- 记录点1：
- 将原始 cross attn 替换为 CBAT（含二维重排与块因果掩码、硬 Top‑k、自适应损失），并在 `PRISMBackbone` 中合并 `[P;E]` 输入以实现 `E←P`，掩码屏蔽 `E→E`。
- 上次问题：
  - 硬 Top‑k 导致未选 token 输出为零，排名断裂，AUPR/AUC下滑。
  - 二维重排引入网格化假设与硬边界，损害序列语义与长程对齐。
  - 门控饱和（Sigmoid靠近0/1）使概率整体上移，Recall↑ Precision↓。
  - 仅单向 `E←P` 缺少补偿，跨域稳健性不足。
- 上次结论：需要替代2D重排、软化稀疏、门控温控、小残差、锚点引导与双向协同，以提升未见细胞上的排序稳健性与AUPR/AUC。

- 记录点2：
  - 用一维多尺度卷积金字塔替代二维重排，输出保持 `[B,L,D]`。
  - `RoPE_CausalBlockAttention` 增加 `use_block_mask/window_size`，默认窗口掩码，弱化硬边界。
  - `RoPE_AdaptAttention` 改为温度软稀疏（列偏置），不再零填未选列，保留排序连续性。
  - `gate_1/gate_2` 加入温度控制与小系数残差，抑制概率上移。
  - `PRISMBackbone` 在池化处引入 `P` 的锚点温度，降低峰值放大。
- 结果：在若干细胞系上 AUPR/AUC 较旧版上升、与 cross attn 接近或略低；Recall回落、Precision上升，F1与 cross attn 接近。
- 反思：仍需加强全局对齐通道、引入双向协同与锚点参与注意力、进一步约束门控与稀疏，以在未见细胞上提升排序稳健性并超过 cross attn。

**本次修复与反思（新增记录）**
- 记录点3：
  - **Prototype Routing Attention (PRA)**：彻底抛弃直接 Attention，改为 $E \to \text{Proto} \to P$ 的路由机制，引入 $K=64$ 个原型向量。
  - **Local Context Encoder (LCE)**：使用 Depthwise Conv 提取局部 Motif 特征，替代金字塔卷积。
- **本次结果**：
  - AUPR/AUC 较记录点2和Baseline均有下滑（如 GM12878 AUPR 0.66 -> 0.65）。
  - 模型置信度分化严重（倾向于0/1），导致 Ranking 能力下降。
  - 未见细胞系泛化能力未达预期，信息瓶颈导致特异性信号丢失。
- **反思**：
  - **过度压缩**：路由机制引入了强信息瓶颈，丢失了 E-P 互作的序列细节和组合特征。
  - **原型平均化**：原型仅捕获了共性模式，无法处理决定互作的关键稀有突变。
  - **缺乏校验**：缺少 Query-Key 直接校验，导致只要匹配同一原型即判定互作，产生高置信度假阳性。
- **结论**：
  - 必须回归 $E-P$ 直接交互以保留细节。
  - 原型应作为**引导（Guide）**而非**瓶颈（Bottleneck）**。
  - 下一步需设计“双流机制”，结合全局直接交互与原型语义引导。

- 记录点4：
  - **双向CBAT + 锚点参与**：在主干中并行 `E←P` 与 `P←E` 两个跨序分支，并在池化处使用可学习锚点温度与方向门控融合（models/PRISMModel.py:382-447）。
  - **两级交互路径**：多尺度卷积、局部块注意力、软稀疏列偏置与双门控融合的 CBAT（models/layers/attn.py:600-888）。
  - **能量头与域对抗**：用 `FourierEnergyKAN` 生成内势 `U_I`，并以 GRL+判别器清洗域信息（models/PRISMModel.py:259-278, 456-475）。
  - **结果**：AUPR/AUC全面提升，但置信度显著上移，阈值偏高；NHEK 出现 Recall≈1.0 与 Precision≈0.50 的极端失衡。
  - **反思**：列偏置直接加到注意力 logits 使 Key 先验跨 Query 过强；`U_I` 未校准导致概率整体上移；稀疏温度缺少退火与熵约束致单峰化。需改为“偏置轻量化+输出校准+稀疏退火”。
