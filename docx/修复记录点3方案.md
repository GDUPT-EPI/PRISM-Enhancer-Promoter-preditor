# 修复记录点3方案：H-CBAT (Hierarchical Causal Bottleneck Attention Transformer)

> **目标**：彻底重构交互逻辑，从“全连接过拟合”转向“瓶颈泛化”，目标 AUPR 突破 80%，实现跨细胞系鲁棒性飞升。

## 1. 核心反思与范式转移

### 1.1 为什么 Cross Attn 鲁棒性差？
标准 Cross Attention 是 $O(L_E \times L_P)$ 的全连接结构。
- **过拟合噪声**：它允许任何 $E$ 位置与任何 $P$ 位置交互。在数据有限的情况下，模型极易记住“偶然的共现噪声”而非“因果的生物学互作”。
- **缺乏结构约束**：它假设所有 Token 地位平等，但生物学上，只有少数关键区域（如转录因子结合位点、CTCF 锚点）起作用。
- **泛化困境**：未见细胞系的序列变异会导致 Attention Map 剧烈波动，因为模型没有学到稳健的“功能原型”。

### 1.2 为什么之前的修复（CBAT v1/v2）提升有限？
- **v1 (2D重排)**：引入了错误的二维空间假设，破坏了序列语义。
- **v2 (局部卷积+软稀疏)**：
    - **过度局部化**：强行用窗口/卷积限制视野，切断了 E-P 的长程通讯（Enhancer 可以在 1Mb 外起作用）。
    - **路径冗余**：堆叠了太多的 Conv、Gate、MLP，导致梯度弥散，难以训练。
    - **锚点虚设**：Anchors 仅仅是拼接在序列里，没有起到“信息瓶颈”的聚合作用。

### 1.3 新范式：Bottleneck Routing (瓶颈路由)
我们需要一个架构，既能保持**全局视野**（捕捉长程），又能**过滤噪声**（鲁棒性）。
**解决方案**：引入一组**隐式原型（Latent Prototypes）**作为 $E$ 和 $P$ 交互的中介。
- **旧模式**：$E \leftrightarrow P$ （直接对话，噪声大）
- **新模式**：$E \rightarrow \text{Prototypes} \rightarrow P$ （通过公共黑板交流，去噪）

---

## 2. H-CBAT 架构设计

**H-CBAT** 由三个精简但强力的模块组成：

### 模块 1: Local Context Encoder (LCE) —— 轻量级局部感知
**目的**：提取 Motif 级别的局部特征，但不做过深的抽象，保留位置信息。
**实现**：
- 输入：k-mer embbeding
- 操作：
    - `Conv1d(k=7)` + `GELU` + `LN` (捕捉短 Motif)
    - `Conv1d(k=15)` + `GELU` + `LN` (捕捉长 Motif)
    - 拼接 + 投影 -> `[B, L, D]`
- **关键点**：不使用 Pooling 缩小长度，保留全部分辨率，因为精确位置对互作预测至关重要。

### 模块 2: Prototype Routing Attention (PRA) —— 核心交互引擎
**目的**：通过有限的“功能原型”进行交互，强制去噪和泛化。
**定义**：
- 设有一组可学习的参数 **Prototypes** $C \in \mathbb{R}^{K \times D}$，其中 $K \ll L$（例如 $K=64$）。这些原型代表了跨细胞系共享的调控模式（如 "Strong Enhancer", "Silencer", "Insulator" 等）。
- **Step 1: Write (E -> Prototypes)**
    - $E$ 序列中的每个 Token 选择它所属的原型。
    - $A_{write} = \text{Softmax}(\frac{Q_E K_C^T}{\sqrt{D}})$ —— 形状 `[B, L_e, K]`
    - $Z_E = A_{write}^T V_E$ —— 形状 `[B, K, D]`
    - **物理意义**：将冗长的 Enhancer 序列压缩为 $K$ 个功能原型的加权组合。噪声 Token 权重被分散，信号 Token 被聚合。
- **Step 2: Read (Prototypes -> P)**
    - $P$ 序列查询这些被激活的原型。
    - $A_{read} = \text{Softmax}(\frac{Q_P K_{Z_E}^T}{\sqrt{D}})$ —— 形状 `[B, L_p, K]`
    - $Out = A_{read} Z_E$ —— 形状 `[B, L_p, D]`
    - **物理意义**：Promoter 根据自身需求，从“已激活的 Enhancer 功能表征”中读取信息。
- **双向路由**：同时执行 $E \to C \to P$ 和 $P \to C \to E$，最后融合。

### 模块 3: Contrastive Consistency (CC) —— 鲁棒性约束
**目的**：确保 $E$ 和 $P$ 在原型空间的一致性。
**实现**：
- 如果 $E$ 和 $P$ 真的互作，它们激活的原型分布应该是匹配的（或者是互补的，如锁和钥匙）。
- **Loss**：最大化 $Z_E$ 和 $Z_P$ 在互作对上的相似度（SimCLR style），最小化非互作对的相似度。
- 这强迫模型学习到：互作不是偶然的，而是基于共享的功能原型模式。

---

## 3. 具体实施步骤

### 3.1 代码重构计划
1.  **清理冗余**：
    - 移除 `CBAT` 类中复杂的 `Module 1/2` 堆叠。
    - 移除 `RoPE_CausalBlockAttention`（不需要因果块掩码了）。
    - 移除二维重排代码 `_to_2d`。

2.  **新建 `PrototypeRoutingAttention`**：
    - 实现 `Write` 和 `Read` 两个阶段的 Attention。
    - 引入 `num_prototypes` 参数（建议 64 或 128）。
    - 引入 `Clustering Loss`：鼓励 Token 明确地归属到某个 Prototype（降低熵）。

3.  **重写 `PRISMBackbone`**：
    - 替换 `cross_attn_1` 和 `cross_attn_2` 为新的 `PrototypeRoutingAttention`。
    - 增加对比学习 Loss 到总 Loss 中。

### 3.2 预期收益分析
- **AUPR 提升**：通过 Bottleneck 过滤掉大量 False Positives，Precision 将大幅提升。
- **鲁棒性提升**：Prototype 是全局共享参数，不依赖于特定细胞系的序列特异性。当遇到新细胞系（如 NHEK）时，只要其 Motif 构成符合已学的 Prototype 模式，模型就能正确预测。
- **计算效率**：复杂度从 $O(L^2)$ 降为 $O(LK)$，训练速度加快，允许更大的 Batch Size。

---

## 4. 关键代码预览 (伪代码)

```python
class PrototypeRoutingAttention(nn.Module):
    def __init__(self, d_model, num_prototypes=64):
        self.prototypes = nn.Parameter(torch.randn(num_prototypes, d_model))
        
    def forward(self, x_source, x_target):
        # x_source: [B, L_s, D] (如 Enhancer)
        # x_target: [B, L_t, D] (如 Promoter)
        
        # 1. Clustering / Write Phase
        # Source Token 投票给 Prototypes
        # attn_write: [B, K, L_s]
        scores_write = torch.einsum('bld, kd -> bkl', x_source, self.prototypes)
        attn_write = torch.softmax(scores_write, dim=-1) 
        
        # 聚合信息到 Prototypes
        # memory: [B, K, D]
        memory = torch.einsum('bkl, bld -> bkd', attn_write, x_source)
        
        # 2. Reading Phase
        # Target Token 查询 Memory
        # attn_read: [B, L_t, K]
        scores_read = torch.einsum('bld, bkd -> blk', x_target, memory)
        attn_read = torch.softmax(scores_read, dim=-1)
        
        # 输出
        # output: [B, L_t, D]
        output = torch.einsum('blk, bkd -> bld', attn_read, memory)
        
        return output
```

这个方案彻底抛弃了“显式修正 Attention Map”的思路（如加 Mask，加 Bias），而是从**信息流动的拓扑结构**上进行了根本性的改变。
