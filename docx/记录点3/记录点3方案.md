# 修复记录点3方案：H-CBAT (Hierarchical Causal Bottleneck Attention Transformer)

> **目标**：彻底重构交互逻辑，从“全连接过拟合”转向“瓶颈泛化”，目标 AUPR 突破 80%，实现跨细胞系鲁棒性飞升。

## 1. 核心反思与范式转移

### 1.1 为什么 Cross Attn 鲁棒性差？
标准 Cross Attention 是 $O(L_E \times L_P)$ 的全连接结构。
- **过拟合噪声**：它允许任何 $E$ 位置与任何 $P$ 位置交互。在数据有限的情况下，模型极易记住“偶然的共现噪声”而非“因果的生物学互作”。
- **缺乏结构约束**：它假设所有 Token 地位平等，但生物学上，只有少数关键区域（如转录因子结合位点、CTCF 锚点）起作用。
- **泛化困境**：未见细胞系的序列变异会导致 Attention Map 剧烈波动，因为模型没有学到稳健的“功能原型”。

### 1.2 为什么之前的修复（CBAT v1/v2）提升有限？
- **v1 (2D重排)**：引入了错误的二维空间假设，破坏了序列语义。
- **v2 (局部卷积+软稀疏)**：
    - **过度局部化**：强行用窗口/卷积限制视野，切断了 E-P 的长程通讯（Enhancer 可以在 1Mb 外起作用）。
    - **路径冗余**：堆叠了太多的 Conv、Gate、MLP，导致梯度弥散，难以训练。
    - **锚点虚设**：Anchors 仅仅是拼接在序列里，没有起到“信息瓶颈”的聚合作用。

### 1.3 新范式：Bottleneck Routing (瓶颈路由)
我们需要一个架构，既能保持**全局视野**（捕捉长程），又能**过滤噪声**（鲁棒性）。
**解决方案**：引入一组**隐式原型（Latent Prototypes）**作为 $E$ 和 $P$ 交互的中介。
- **旧模式**：$E \leftrightarrow P$ （直接对话，噪声大）
- **新模式**：$E \rightarrow \text{Prototypes} \rightarrow P$ （通过公共黑板交流，去噪）

---

## 2. H-CBAT 架构设计

**H-CBAT** 由三个精简但强力的模块组成：

### 模块 1: Local Context Encoder (LCE) —— 轻量级局部感知
**目的**：提取 Motif 级别的局部特征，但不做过深的抽象，保留位置信息。
**实现**：
- 输入：k-mer embbeding
- 操作：
    - `Conv1d(k=7)` + `GELU` + `LN` (捕捉短 Motif)
    - `Conv1d(k=15)` + `GELU` + `LN` (捕捉长 Motif)
    - 拼接 + 投影 -> `[B, L, D]`
- **关键点**：不使用 Pooling 缩小长度，保留全部分辨率，因为精确位置对互作预测至关重要。

### 模块 2: Prototype Routing Attention (PRA) —— 核心交互引擎
**目的**：通过有限的“功能原型”进行交互，强制去噪和泛化。
**定义**：
- 设有一组可学习的参数 **Prototypes** $C \in \mathbb{R}^{K \times D}$，其中 $K \ll L$（例如 $K=64$）。这些原型代表了跨细胞系共享的调控模式（如 "Strong Enhancer", "Silencer", "Insulator" 等）。
- **Step 1: Write (E -> Prototypes)**
    - $E$ 序列中的每个 Token 选择它所属的原型。
    - $A_{write} = \text{Softmax}(\frac{Q_E K_C^T}{\sqrt{D}})$ —— 形状 `[B, L_e, K]`
    - $Z_E = A_{write}^T V_E$ —— 形状 `[B, K, D]`
    - **物理意义**：将冗长的 Enhancer 序列压缩为 $K$ 个功能原型的加权组合。噪声 Token 权重被分散，信号 Token 被聚合。
- **Step 2: Read (Prototypes -> P)**
    - $P$ 序列查询这些被激活的原型。
    - $A_{read} = \text{Softmax}(\frac{Q_P K_{Z_E}^T}{\sqrt{D}})$ —— 形状 `[B, L_p, K]`
    - $Out = A_{read} Z_E$ —— 形状 `[B, L_p, D]`
    - **物理意义**：Promoter 根据自身需求，从“已激活的 Enhancer 功能表征”中读取信息。
- **双向路由**：同时执行 $E \to C \to P$ 和 $P \to C \to E$，最后融合。

### 模块 3: Contrastive Consistency (CC) —— 鲁棒性约束
**目的**：确保 $E$ 和 $P$ 在原型空间的一致性。
**实现**：
- 如果 $E$ 和 $P$ 真的互作，它们激活的原型分布应该是匹配的（或者是互补的，如锁和钥匙）。
- **Loss**：最大化 $Z_E$ 和 $Z_P$ 在互作对上的相似度（SimCLR style），最小化非互作对的相似度。
- 这强迫模型学习到：互作不是偶然的，而是基于共享的功能原型模式。

---

## 3. 具体实施步骤

### 3.1 代码重构计划
1.  **清理冗余**：
    - 移除 `CBAT` 类中复杂的 `Module 1/2` 堆叠。
    - 移除 `RoPE_CausalBlockAttention`（不需要因果块掩码了）。
    - 移除二维重排代码 `_to_2d`。

2.  **新建 `PrototypeRoutingAttention`**：
    - 实现 `Write` 和 `Read` 两个阶段的 Attention。
    - 引入 `num_prototypes` 参数（建议 64 或 128）。
    - 引入 `Clustering Loss`：鼓励 Token 明确地归属到某个 Prototype（降低熵）。

3.  **重写 `PRISMBackbone`**：
    - 替换 `cross_attn_1` 和 `cross_attn_2` 为新的 `PrototypeRoutingAttention`。
    - 增加对比学习 Loss 到总 Loss 中。

### 3.2 预期收益分析
- **AUPR 提升**：通过 Bottleneck 过滤掉大量 False Positives，Precision 将大幅提升。
- **鲁棒性提升**：Prototype 是全局共享参数，不依赖于特定细胞系的序列特异性。当遇到新细胞系（如 NHEK）时，只要其 Motif 构成符合已学的 Prototype 模式，模型就能正确预测。
- **计算效率**：复杂度从 $O(L^2)$ 降为 $O(LK)$，训练速度加快，允许更大的 Batch Size。

---

## 4. 关键代码预览 (伪代码)

```python
class PrototypeRoutingAttention(nn.Module):
    def __init__(self, d_model, num_prototypes=64):
        self.prototypes = nn.Parameter(torch.randn(num_prototypes, d_model))
        
    def forward(self, x_source, x_target):
        # x_source: [B, L_s, D] (如 Enhancer)
        # x_target: [B, L_t, D] (如 Promoter)
        
        # 1. Clustering / Write Phase
        # Source Token 投票给 Prototypes
        # attn_write: [B, K, L_s]
        scores_write = torch.einsum('bld, kd -> bkl', x_source, self.prototypes)
        attn_write = torch.softmax(scores_write, dim=-1) 
        
        # 聚合信息到 Prototypes
        # memory: [B, K, D]
        memory = torch.einsum('bkl, bld -> bkd', attn_write, x_source)
        
        # 2. Reading Phase
        # Target Token 查询 Memory
        # attn_read: [B, L_t, K]
        scores_read = torch.einsum('bld, bkd -> blk', x_target, memory)
        attn_read = torch.softmax(scores_read, dim=-1)
        
        # 输出
        # output: [B, L_t, D]
        output = torch.einsum('blk, bkd -> bld', attn_read, memory)
        
        return output
```

这个方案彻底抛弃了“显式修正 Attention Map”的思路（如加 Mask，加 Bias），而是从**信息流动的拓扑结构**上进行了根本性的改变。


# H-CBAT (Hierarchical Causal Bottleneck Attention Transformer) 方案

## 1. 核心范式转移：从 "全连接交互" 到 "原型路由"

### 现状与痛点 (Standard Cross Attention & Old CBAT)
- **Standard Cross Attention**: 是一种全连接图 ($O(N \times M)$)，Enhancer 的每一个 token 都试图与 Promoter 的每一个 token 交互。
  - **问题**: 大部分 E-P 位点对是无关的（噪音），强制全连接引入了大量假阳性信号。
  - **结果**: 模型容易过拟合训练集中的特异性噪音，导致跨细胞系（Unseen Cell Line）泛化能力差 (AUPR 卡在 ~66%)。
- **Old CBAT**: 试图通过 Causal Mask 和 RoPE 引入位置偏置，通过 Gate 机制筛选。
  - **问题**: 仍然基于“点对点”的交互逻辑，只是加了限制。Gate 机制在噪音高时容易失效，且 RoPE 在长距离交互中可能带来不必要的强位置约束，限制了远距离调控的发现。

### H-CBAT 的核心思想
**"不直接交互，而是通过共享的'转录因子原型 (TF Prototypes)'进行通信。"**

我们将交互过程解耦为两个阶段，引入一个**全局共享的、可学习的瓶颈层 (Bottleneck)** —— **Prototypes**。
1.  **Write Phase (E -> Prototypes)**: Enhancer 将其局部 Motif 信息“写入”到全局原型空间。即：这段 Enhancer 序列激活了哪些转录因子模式？
2.  **Read Phase (Prototypes -> P)**: Promoter 从原型空间“读取”相关信息。即：Promoter 需要哪些转录因子模式来启动转录？

**优势**:
- **降噪 (Denoising)**: 无关的背景序列无法激活原型，噪音被过滤。
- **鲁棒性 (Robustness)**: 原型代表了跨细胞系通用的生物学模式 (如 TF Binding Motifs)，而非特定细胞系的噪音。
- **稀疏性 (Sparsity)**: 只有匹配的模式才会建立通路，模拟了生物学上的特异性结合。

---

## 2. H-CBAT 架构设计

H-CBAT 由三个核心模块组成：

### 2.1 LCE (Local Context Encoder) - 局部上下文编码器
- **目的**: 在进入交互层之前，先提取 Motif 级别的局部特征，保留位置信息，**不做下采样**。
- **结构**:
  - **Parallel Multi-Scale CNN**:
    - 分支1: Kernel=7 (短 Motif) -> Depthwise Conv + Pointwise Conv (轻量化)
    - 分支2: Kernel=15 (长 Motif/复合物) -> Depthwise Conv + Pointwise Conv
  - **Residual Connection**: $x_{out} = \text{Norm}(x_{in} + \text{Proj}([f_{short}, f_{long}]))$
- **作用**: 将原始的 DNA Embedding 转化为具有生物学语义的 Motif Embedding。

### 2.2 PRA (Prototype Routing Attention) - 原型路由注意力
这是核心交互引擎，替代原来的 `nn.MultiheadAttention`。

- **定义**:
  - $E \in \mathbb{R}^{L_e \times D}$: Enhancer 特征 (来自 LCE)
  - $P \in \mathbb{R}^{L_p \times D}$: Promoter 特征 (来自 LCE)
  - $C \in \mathbb{R}^{K \times D}$: **Learnable Prototypes (全局共享参数)**, $K \ll L_e, L_p$ (e.g., $K=64$)

- **流程 (双向路由)**:
  
  **方向 1: Enhancer -> Promoter (E 激活 P)**
  1.  **Write (Clustering)**: Enhancer 查询 Prototypes。
      - $Q=C, K=E, V=E$
      - $A_{write} = \text{Softmax}(C E^T / \sqrt{D})$
      - $M_e = A_{write} E \in \mathbb{R}^{K \times D}$ (把 E 的信息聚合到 K 个原型上)
      - *Clustering Loss*: 鼓励 E 的每个 token 明确归属到少数几个 Prototype (降低熵)。
  2.  **Read (Retrieval)**: Promoter 查询 Updated Prototypes ($M_e$)。
      - $Q=P, K=M_e, V=M_e$
      - $A_{read} = \text{Softmax}(P M_e^T / \sqrt{D})$
      - $P_{out} = A_{read} M_e \in \mathbb{R}^{L_p \times D}$ (Promoter 接收到的调控信号)

  **方向 2: Promoter -> Enhancer (P 寻找 E)** (对称操作)
  - 同理，通过 Prototypes 传递 P 的需求到 E。

### 2.3 CC (Contrastive Consistency) - 对比一致性
- **目的**: 确保 Enhancer 和 Promoter 在原型空间的表征是一致的（如果是正样本）。
- **Loss**:
  - 对于正样本对 $(E, P)$，它们激活的原型分布应该相似。
  - $L_{cc} = \text{MSE}(M_e, M_p)$ 或 $L_{cc} = \text{KL}(A_{write\_E} || A_{write\_P})$
  - 这里我们简化为 **Clustering Loss**，即强迫 token 聚类，间接提高一致性。

---

## 3. 预期效果与指标
- **AUPR**: 目标 **80%** (当前 ~66%)。
  - 来源：通过 Bottleneck 过滤掉 90% 的无关交互噪音。
- **AUC**: 保持或提升。
- **Unseen Cell Line**: 显著提升。
  - 来源：Prototypes 学习的是通用的 TF 模式，而非细胞系特有的 Artifacts。

## 4. 代码实现计划
1.  **重构 `attn.py`**:
    - 移除旧 `CBAT` 类。
    - 新增 `LocalContextEncoder` 类。
    - 新增 `PrototypeRoutingAttention` 类 (含 Write/Read 逻辑)。
2.  **更新 `PRISMModel.py`**:
    - `PRISMBackbone` 中替换 `self.cross_attn` 为 `PrototypeRoutingAttention`。
    - 引入 `Clustering Loss` 到总 Loss 中。
3.  **超参**:
    - `num_prototypes = 64`
    - `d_model = 128` (保持不变)
