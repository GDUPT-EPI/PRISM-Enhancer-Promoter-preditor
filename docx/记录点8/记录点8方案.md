# 记录点8方案：强制上下文依赖与元学习范式重构

## 1. 核心理念：Context-First（先上下文，后判断）

针对记录点7中"单样本泄露"和"随机采样噪声"导致的失败，本方案提出根本性的架构与训练流重构。我们将放弃"模型自己在杂乱数据中寻找上下文"的幻想，转而采用**元学习（Meta-Learning）**的思想，强制模型依赖上下文进行推断。

**核心公式修正**：
- 旧（错误）：$y = f(x_i, \text{Context}(Batch))$ —— 上下文只是辅助，没有它 $x_i$ 也能预测。
- 新（正确）：$y = f(x_i | \text{Environment}(\{x_j\}_{j \in S}))$ —— 必须先根据支持集 $S$ 推断环境 $E$，再在环境 $E$ 中预测 $x_i$。

**区别于记录点4的本质**：
- 记录点4依赖于 GRL 清洗后的 $U_I$ 作为保守估计，在 OOD 上只能给出一种“通用解”，无法应对细胞特异性差异，因此 AUPR 卡在 70%。
- 记录点8通过**蒸馏学习**，强制 Student 模仿 Teacher（上帝视角），学会从 EP 集的分布（Support Set）中反推环境阻抗 $R_E$。这使得模型在 OOD 上能够像变色龙一样，根据观察到的样本分布动态调整预测策略，从而实现真正的鲁棒性和准确性。

## 2. 架构重构：环境-势能解耦 (Environment-Potential Decoupling)

我们将模型明确划分为两个解耦的流，并**重新定义 Teacher 的角色与观测方式**。

### 2.1 固有势能流 (Intrinsic Potential Stream)
- **输入**：单条样本 $(E_i, P_i)$
- **功能**：计算不依赖于细胞环境的物理化学结合势能 $U_I$。
- **模块**：保留现有的 `PRISMBackbone` + `FourierEnergyKAN`。
- **约束**：此路径**严禁**接触任何 Batch 信息或 Cell ID 信息。

### 2.2 环境阻抗流 (Environmental Resistance Stream)
这是本次重构的核心。

#### 2.2.1 Teacher：基于原型图的流形观测者 (Manifold Observer via Prototype Graph)
我们不能想当然地认为 Teacher 仅仅通过 Cell ID 查表就能得到完美的环境阻抗。环境是复杂的，甚至同一细胞系内部也存在异质性。
- **模块来源**：参考废案 `models/layers/graph_context.py` 中的 `GraphContext`。
- **输入**：Cell ID Embedding + 全局 EP 原型图（Global EP Prototype Graph）。
- **机制**：
  - Teacher 维护一个全局的**细胞系原型图 (Cell Prototype Graph)**，节点为各细胞系的原型向量 $\{\mu_c\}$。
  - Teacher 并不直接输出 $R_E$，而是根据当前 Cell ID 在流形上定位，通过 **GCN (Graph Convolutional Network)** 聚合邻居细胞系的信息，生成一个平滑的、经过流形校准的**环境分布特征 $M_T$**。
  - **核心洞察**：Teacher 的优势在于它能看到**细胞系之间的关系**（如 GM12878 和 K562 的相似性），从而构建一个连续的环境流形，而不是离散的孤岛。

#### 2.2.2 Student：基于集合的分布推断者 (Distribution Inferer via Set Aggregation)
Student 没有上帝视角（Cell ID），它只能看到眼前的样本。
- **输入**：Support Set（来自同一细胞系的一组样本 $\{x_j\}$）。
- **模块**：**DeepSet Encoder** 或 **ISAB**。
- **功能**：
  - Student 必须从 Support Set 中提取出样本在 Latent Space 的分布特征（均值、方差、高阶矩）。
  - **推断逻辑**：Student 观察一组 $(E, P)$ 对，发现某些在物理上应该结合（$U_I$ 高）的 Pair 在当前 Support Set 中并没有出现或者标签为负，从而推断出当前环境存在特定的阻抗 $R_E$。
  - **对齐目标**：Student 的目标不是模仿 Teacher 的输出值，而是模仿 Teacher 的**分布特征 $M_T$**。即 Student 需要学会：**“什么样的样本集合分布对应于流形上的哪一个位置？”**

- **关键机制**：
  - **同质化采样 (Homogeneous Sampling)**：训练时，每一个 Batch（或 Sub-batch）必须由来自**同一细胞系**的样本组成。
  - **Masked Prediction**：预测 Target $x_i$ 时，必须将其从 Support Set 中移除（或掩码），防止标签泄露。

### 2.3 最终判决 (The Verdict)
$$ P(y=1|x_i, S) = \sigma\left(\frac{U_I(x_i) - R_E(S)}{T}\right) $$
- 只有当 $U_I$ 超过环境阻力 $R_E$ 时，互作才会发生。
- 如果 $R_E$ 无法正确推断（例如 Support Set 为空或全零），$R_E$ 应趋向于一个默认的高阻尼值，导致预测趋于保守（Low Precision, Low Recall），而不是盲目自信。

## 3. 训练流程重构：Episodic Training

### 3.1 数据加载器改造
- 废弃 `RandomBatchSampler`。
- 实现 `CellLineBatchSampler`：
  - 每次迭代随机选择一个细胞系 $C$。
  - 从 $C$ 中采样 $N$ 个样本构成一个 Batch。

### 3.2 蒸馏策略调整 (Distillation 2.0)
- **Teacher**：通过 `GraphContext` (GCN) 在细胞系流形上生成 $M_T$。
- **Student**：通过 `ISAB` 从 Support Set 生成 $M_S$。
- **Loss**：
  1. **Prediction Loss**: $\mathcal{L}_{BCE}(y, \sigma(U_I - R_E^{Pred}))$
  2. **Manifold Alignment Loss (New)**: 
     $$ \mathcal{L}_{align} = \| M_S(SupportSet) - M_T(CellID) \|_2^2 $$
     强制 Student 将观测到的经验分布映射到 Teacher 构建的理论流形上。
  3. **Consistency Loss**: 同一个细胞系的不同 Batch，生成的 $M_S$ 应当相似。

## 4. 预测脚本适配
- 在 `predict.py` 中，对于测试集中的每一个细胞系：
  1. 收集该细胞系的所有样本（或随机抽取一个子集作为 Support Set）。
  2. 输入 Student 模块计算该细胞系的全局 $R_E$。
  3. 使用该 $R_E$ 对所有样本进行批量预测。
- 对于完全未知的细胞系，允许用户提供少量无标签样本作为 Support Set（Few-Shot Unsupervised Adaptation）。

## 5. 预期收益
- **鲁棒性**：彻底解决"单样本泄露"问题，模型在未知细胞系上的表现将严格依赖于其对 Support Set 分布的理解。
- **可解释性**：$U_I$ 代表分子亲和力，$R_E$ 代表染色质开放性/环境抑制，两者泾渭分明。
- **AUPR 突破**：通过正确的环境建模，消除由噪声引起的 False Positives，提升 Precision。
