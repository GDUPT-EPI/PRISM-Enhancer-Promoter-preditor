# CBAT 修复与反思记录

## 1. 分析修复记录点2：修改前 vs 修改后

### 修改前（修复记录点1版本）
- **核心机制**：
  - **二维重排**：将一维序列 `[B, L, D]` 强行重排为 `[B, D, H, W]` 的“图像”，引入了不自然的二维空间邻域假设（网格化假设）。
  - **硬 Top-k 稀疏**：在注意力中使用硬截断（Hard Top-k），未选中的 Token 输出强制为零。
  - **单向信息流**：仅实现了 `E←P` 的单向增强，缺乏双向协同。
  - **门控饱和**：Sigmoid 门控容易饱和在 0 或 1，导致梯度消失或概率值整体漂移。

- **问题表现**：
  - **排序断裂**：硬 Top-k 导致大量低分 Token（可能是负样本或弱正样本）的梯度和输出直接被切断，模型无法学习细粒度的排序关系，导致 AUPR/AUC 下滑。
  - **语义破坏**：二维重排破坏了 DNA 序列的一维连续性和长程依赖，引入了错误的局部相关性（本来不相邻的碱基在 2D 网格中可能相邻）。
  - **泛化差**：在未见过的细胞系（如 NHEK）上表现尤为糟糕，说明模型过拟合了特定的“图像”模式，而非生物学规律。

### 修改后（修复记录点2版本）
- **核心改进**：
  - **一维多尺度卷积金字塔**：替代了二维重排，使用 `kernel_size=[7, 15, 31]` 的一维卷积提取多尺度局部特征，保持了序列的一维结构 `[B, L, D]`。
  - **温度软稀疏（Soft Sparsity）**：在 `RoPE_AdaptAttention` 中使用带温度系数的 Softmax/Sparsemax，不再直接置零，而是给予低分项极小的非零权重，保留了梯度的传导和排序的连续性。
  - **双向协同**：在 `PRISMBackbone` 中引入了 `P←E` 和 `E←P` 的双向交叉注意力路径，并增加了锚点（Anchors）作为中介。
  - **门控优化**：引入温度控制（`temp.clamp_min(0.5)`）和小系数残差，防止门控过早饱和。
  - **因果/滑窗掩码**：在 `RoPE_CausalBlockAttention` 中加入了 `window_size`，限制了注意力的局部范围，减少无关噪声干扰。

- **结果变化**：
  - **指标回升**：相比点1版本，AUPR 和 AUC 在大多数细胞系上有显著回升（如 Stomach AUPR 0.6772 -> 0.7076）。
  - **Precision 提升**：Precision 普遍大幅提升（如 GM12878 0.5368 -> 0.6082），说明减少了大量的假阳性。
  - **Recall 下降**：Recall 从点1的极高值（>0.9）回落到正常范围（~0.7-0.8），说明模型不再盲目预测正类（之前的 Recall 虚高是因为预测值整体偏大）。
  - **未见细胞系改善**：NHEK（未见）的 AUPR 从 0.6307 提升到 0.6403，泛化能力有所增强。

---

## 2. 分析与标准 Cross Attn 的区别

| 特性 | 标准 Cross Attn (Baseline) | CBAT (当前版本) | 区别影响分析 |
| :--- | :--- | :--- | :--- |
| **序列结构** | 纯序列，全连接注意力 | 序列 + 多尺度卷积 + 局部滑窗/块掩码 | CBAT 引入了归纳偏置（局部性、多尺度），理论上更适合生物序列，但也增加了参数和优化难度。 |
| **稀疏策略** | 无（全 Softmax） | 自适应软稀疏 (Adaptive Soft Sparsity) | 标准 Attention 会关注全局所有 Token，噪声多；CBAT 试图只关注关键区域，但如果稀疏策略不准，反而会丢掉重要信息。 |
| **位置编码** | RoPE (标准) | RoPE + 结构/位置调制 (Structure/Pos Modulation) | CBAT 的位置调制过于复杂，可能引入了错误的先验，干扰了 RoPE 本身对相对位置的编码。 |
| **交互方式** | 直接 `Q(E) - K(P) - V(P)` | 多阶段：卷积 -> Causal Block Attn -> Adaptive Attn -> FFN | CBAT 路径过长，虽然非线性能力强，但也容易导致梯度在深层传播中衰减或弥散，且难以训练。 |
| **损失函数** | 仅分类损失 | 分类损失 + 自适应稀疏损失 (Adaptive Loss) + 投机惩罚 | 多任务学习如果权重平衡不好（如 Adaptive Loss 过大），会干扰主任务（分类）的优化方向。 |
| **参数量** | 较小，轻量级 | 较大（包含多个卷积、MLP、门控、字典基向量） | 参数量大导致更容易过拟合训练集，而在未见细胞系上泛化受限。 |

**核心区别总结**：标准 Cross Attn 简单直接，依赖 Transformer 自身的全局建模能力；CBAT 试图通过人为设计的复杂机制（稀疏、局部卷积、门控）来“教”模型如何关注重点，但这些机制引入了额外的超参数和假设，若未精细调优，反而可能成为阻碍。

---

## 3. 分析为什么提升不明显（甚至部分指标不如 Baseline）

尽管修复点2修复了严重的逻辑错误（如硬截断），但结果仍未全面超越 Baseline，原因可能如下：

1.  **过度设计的“局部性”限制了全局视野**：
    *   CBAT 引入了 `ms_convs`（多尺度卷积）和 `window_size`（滑窗掩码），强制模型关注局部。然而，增强子-启动子互作（E-P loop）本质上是**长程相互作用**。过强的局部偏置可能切断了关键的远距离信号，而标准 Cross Attn 是全连接的，没有这个限制。

2.  **稀疏注意力（Adaptive Attention）的“双刃剑”**：
    *   虽然改为软稀疏，但 `sparsity_target` 仍试图让分布变得尖锐。对于 E-P 互作，可能存在**弥散的微弱信号**累积效应，而非仅依赖少数几个强 Token。强制稀疏化可能丢弃了这些通过“集体效应”起作用的弱信号，导致 Recall 不如全连接的 Baseline。

3.  **复杂的“模块堆叠”导致优化困难**：
    *   CBAT 结构复杂：`Conv -> CausalAttn -> Gate -> AdaptAttn -> FFN -> Gate`。
    *   **深度过深**：相比 Baseline 的单层或双层 Cross Attn，CBAT 的路径太长，且包含多个 Sigmoid 门控。尽管加了残差，但在反向传播时，梯度流经多个非线性变换，容易变得杂乱或衰减，导致收敛到次优解。
    *   **门控干扰**：`Gate` 的存在本意是融合特征，但如果门控网络（MLP）未能快速学好，早期的随机门控值会给信号引入巨大的乘性噪声，阻碍主干网络的学习。

4.  **位置编码的冲突**：
    *   `RoPE_AdaptAttention` 中同时使用了标准的 RoPE 和自定义的 `_position_modulation`（基于频率的显式位置偏置）。这两者可能存在功能重叠甚至冲突，导致位置信息编码混乱。

5.  **锚点（Anchors）机制未充分利用**：
    *   虽然引入了 Anchors，但在代码中 Anchors 只是作为输入序列的一部分拼接进去（`combined = torch.cat([pr, anchors, enh], dim=0)`），并没有专门的机制强迫 Anchors 聚合全局信息后再分发。如果 Attention 机制没有学会“利用”Anchors，它们就只是增加了序列长度的噪声。

6.  **投机惩罚与自适应损失的权衡**：
    *   `SpeculationPenaltyLoss` 和 `AdaptiveIMMAXLoss` 以及 `_adaptive_loss` 同时作用。过多的正则化项可能让模型“畏手畏脚”，为了降低辅助损失（如稀疏度惩罚）而牺牲了分类的准确性（主任务）。

**结论**：CBAT 目前是“为了鲁棒性而牺牲了直接性”，引入的机制虽然理论上能提高抗噪能力，但破坏了信号传输的直接通路。要超越 Baseline，必须**做减法**，回归“长程交互”的本质，同时保留“软稀疏”来去噪，但不能强行限制局部感受野。
